{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers.createCsv()\n",
    "data = pd.read_csv(\"17k-tweets.csv\")\n",
    "\n",
    "features = data.iloc[:,0].values\n",
    "labels = data.iloc[:,1].values\n",
    "\n",
    "stop_words = helpers.get_external_stopwords()\n",
    "features = helpers.cleanFeatures(features, stop_words)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "labels = le.fit_transform(labels) \n",
    "labels = to_categorical(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-cff5813b0c68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmaxlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_max_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 1\n",
    "MAX_NB_WORDS = 2500\n",
    "\n",
    "# Split train & test\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# Tokenize and transform to integer index\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "def find_max_length(features):\n",
    "    length = 0    \n",
    "    for sentence in features:\n",
    "        if len(sentence) > length:\n",
    "            length = len(sentence)\n",
    "    return length\n",
    "\n",
    "maxlen = find_max_length(x_train)\n",
    "print (maxlen)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'VOCAB_SIZE' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c5d1f4390420>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEMBEDING_DIM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VOCAB_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "# Define CNN architecture\n",
    "\n",
    "\n",
    "EMBEDING_DIM = 100\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 200\n",
    "filters = 20\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(VOCAB_SIZE, EMBEDING_DIM, input_length=maxlen))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(layers.Dense(3, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    batch_size=BATCH_SIZE)\n",
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define CNN architecture\n",
    "# from keras.layers import Dense, Dropout, Activation\n",
    "# from keras.layers import Embedding\n",
    "# from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "\n",
    "# EMBEDING_DIM = 100\n",
    "# EPOCHS = 8\n",
    "# BATCH_SIZE = 200\n",
    "# filters = 20\n",
    "# kernel_size = 3\n",
    "# hidden_dims = 250\n",
    "\n",
    "# # # channel 1\n",
    "# # \tinputs1 = Input(shape=(length,))\n",
    "# # \tembedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "# # \tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "# # \tdrop1 = Dropout(0.5)(conv1)\n",
    "# # \tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "# # \tflat1 = Flatten()(pool1)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(layers.Embedding(VOCAB_SIZE, EMBEDING_DIM, input_length=maxlen))\n",
    "# # model.add(Dropout(0.5))\n",
    "# model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(layers.MaxPooling1D(pool_size=2))\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "# model.add(layers.Dense(3, activation='sigmoid'))\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# history = model.fit(x_train, y_train,\n",
    "#                     epochs=EPOCHS,\n",
    "#                     verbose=1,\n",
    "#                     validation_data=(x_test, y_test),\n",
    "#                     batch_size=BATCH_SIZE)\n",
    "# loss, accuracy = model.evaluate(x_train, y_train, verbose=1)\n",
    "# print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "# loss, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "# print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}