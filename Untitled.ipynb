{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy for C=0.01: 0.555\nAccuracy for C=0.05: 0.5716666666666667\nAccuracy for C=0.25: 0.56\nAccuracy for C=0.5: 0.565\nAccuracy for C=1: 0.5633333333333334\nFinal Accuracy: 0.585\n"
    }
   ],
   "source": [
    "from dataScript import getDataset\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "data = getDataset()\n",
    "X = data.iloc[:,0].values\n",
    "y = data.iloc[:,1].values\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"[.,\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(\\-)|(\\/)\")\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "    return reviews\n",
    "\n",
    "X_train = preprocess_reviews(X_train)  \n",
    "X_test = preprocess_reviews(X_test)\n",
    "\n",
    "\n",
    "turkish_stop_words = stopwords.words('turkish')\n",
    "def remove_stop_words(corpus):\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split() \n",
    "                      if word not in turkish_stop_words])\n",
    "        )\n",
    "    return removed_stop_words\n",
    "\n",
    "no_stop_words = remove_stop_words(X_train)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "ngram_vectorizer.fit(X_train)\n",
    "X = ngram_vectorizer.transform(X_train)\n",
    "X_test = ngram_vectorizer.transform(X_test)\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y_train, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    svm = LinearSVC(C=c)\n",
    "    svm.fit(X_tr, y_tr)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, svm.predict(X_val))))\n",
    "    \n",
    "# Accuracy for C=0.01: 0.89104\n",
    "# Accuracy for C=0.05: 0.88736\n",
    "# Accuracy for C=0.25: 0.8856\n",
    "# Accuracy for C=0.5: 0.88608\n",
    "# Accuracy for C=1: 0.88592\n",
    "    \n",
    "final_svm_ngram = LinearSVC(C=0.01)\n",
    "final_svm_ngram.fit(X, y_train)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(y_test, final_svm_ngram.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = data.iloc[:,0].values\n",
    "#ps = PorterStemmer()\n",
    "#print(X[2])\n",
    "#stop_words = set(stopwords.words('turkish')) #read from Talha's file\n",
    "#print(len(stop_words))\n",
    "#words = word_tokenize(X[2])\n",
    "#filtered_sens = [w for w in words if not w in stop_words]\n",
    "#print(filtered_sens)\n",
    "#print(X[2])\n",
    "#print(word_tokenize(X[2]))\n",
    "# for w in words:\n",
    "#     print(ps.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "13723"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from dataScript import getDataset\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words=()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\", \"\", token)\n",
    "\n",
    "        # if tag.startswith(\"NN\"):\n",
    "        #     pos = 'n'\n",
    "        # elif tag.startswith('VB'):\n",
    "        #     pos = 'v'\n",
    "        # else:\n",
    "        #     pos = 'a'\n",
    "\n",
    "        # lemmatizer = WordNetLemmatizer()\n",
    "        # token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "def get_tokens(sentences):\n",
    "    tokens = []\n",
    "    for sen in sentences:\n",
    "        words = (word_tokenize(sen))\n",
    "        for w in words:\n",
    "            tokens.append(w)\n",
    "    return tokens      \n",
    "\n",
    "\n",
    "data = getDataset()\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "X = data.iloc[:,0].values\n",
    "y = data.iloc[:,1].values\n",
    "y = list(map(str, y))\n",
    "for i, val in enumerate(y): \n",
    "    if val == '1':\n",
    "        y[i] = 'pos'\n",
    "    elif val == '2':\n",
    "        y[i] = 'neg'\n",
    "    else:\n",
    "        y[i] = 'not'\n",
    "\n",
    "stop_words = stopwords.words('turkish')\n",
    "tweet_tokens = get_tokens(X)\n",
    "no_repeat = set(tweet_tokens)\n",
    "len(no_repeat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['1']\n"
    }
   ],
   "source": [
    "from dataScript import getDataset\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "\n",
    "\n",
    "data = getDataset()\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "X = data.iloc[:,0].values\n",
    "y = data.iloc[:,1].values\n",
    "y = list(map(str, y))\n",
    "for i, val in enumerate(y): \n",
    "    if val == '3':\n",
    "        np.delete(X, i)\n",
    "        np.delete(y, i)          \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='word', lowercase=True)\n",
    "sent_train_vector = vectorizer.fit_transform(X)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "model = classifier.fit(X=sent_train_vector.toarray(), y=y)\n",
    "\n",
    "sent_test_vector = vectorizer.transform(['kotu'])\n",
    "y_prediction = model.predict(sent_test_vector.toarray())\n",
    "print (y_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.585\n0.56\n0.57\n0.57\n0.54\n0.565\n"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from dataScript import getDataset\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = getDataset()\n",
    "acc = 0\n",
    "for j in range(5):\n",
    "  data = data.sample(frac=1).reset_index(drop=True)\n",
    "  data.head()\n",
    "  #data.Sentiment.value_counts().plot(kind='pie', autopct='%1.0f%%', colors=[\"red\", \"yellow\", \"green\"])\n",
    "\n",
    "  features = data.iloc[:,0].values\n",
    "  labels = data.iloc[:,1].values\n",
    "  f = open(\"stop_words.txt\", \"r\", encoding='utf8')\n",
    "  stop = []\n",
    "  for x in f:\n",
    "    stop.append(x.replace('\\n',''))\n",
    "  f.close()\n",
    "  processed_features = []\n",
    "  ss = []\n",
    "\n",
    "  for i in range(0, len(stop)):\n",
    "\n",
    "      s = str(stop[i])\n",
    "      s = s.lower()\n",
    "      s= re.sub(r'ü', 'u', s)\n",
    "      s= re.sub(r'ö', 'o', s)\n",
    "      s= re.sub(r'ı', 'i', s)\n",
    "      s= re.sub(r'ğ', 'g', s)\n",
    "      s= re.sub(r'ç', 'c', s)\n",
    "      ss.append(s)\n",
    "\n",
    "  for sentence in range(0, len(features)):\n",
    "      processed_feature = re.sub(r'\\s\\'\\s', '\\'', str(features[sentence]))\n",
    "      processed_feature= re.sub(r'[.!?,\\\"\\[\\]]', '', processed_feature)\n",
    "      \n",
    "      #processed_feature= re.sub(r'(?<=\\d)(\\'\\s)', '\\'', processed_feature)    \n",
    "      processed_feature = processed_feature.lower()\n",
    "      processed_feature= re.sub(r'ü', 'u', processed_feature)\n",
    "      processed_feature= re.sub(r'ö', 'o', processed_feature)\n",
    "      processed_feature= re.sub(r'ı', 'i', processed_feature)\n",
    "      processed_feature= re.sub(r'ğ', 'g', processed_feature)\n",
    "      #processed_feature= re.sub(r'ş', 's', processed_feature)\n",
    "      processed_feature= re.sub(r'ç', 'c', processed_feature)\n",
    "      # Replaces #hashtag with hashtag\n",
    "      processed_feature = re.sub(r'#(\\S+)', r' \\1 ', processed_feature)\n",
    "\n",
    "      # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "      processed_feature = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', processed_feature)\n",
    "      # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "      processed_feature = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', processed_feature)\n",
    "      # Love -- <3, :*\n",
    "      processed_feature = re.sub(r'(<3|:\\*)', ' EMO_POS ', processed_feature)\n",
    "      # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "      processed_feature = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', processed_feature)\n",
    "      # Sad -- :-(, : (, :(, ):, )-:\n",
    "      processed_feature = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', processed_feature)\n",
    "      # Cry -- :,(, :'(, :\"(\n",
    "      processed_feature = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', processed_feature)\n",
    "\n",
    "      # Convert more than 2 letter repetitions to 2 letter\n",
    "      processed_feature = re.sub(r'(.)\\1+', r'\\1\\1', processed_feature)\n",
    "      processed_features.append(processed_feature)\n",
    "  #print(processed_features[10:])\n",
    "  # all_words = []\n",
    "  # tknzr = TweetTokenizer()\n",
    "  # for sen in processed_features:\n",
    "  #     words = tknzr.tokenize(sen)    \n",
    "  #     #print(words)\n",
    "  #     filtered_words = [w for w in words if not w in stop]\n",
    "  #     all_words.append(str(filtered_words))\n",
    "  #     #print(filtered_words)\n",
    "  \n",
    "  vectorizer = TfidfVectorizer (max_features=2500, min_df=2, max_df=0.5, stop_words=stop)\n",
    "  processed_features = vectorizer.fit_transform(processed_features)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=1\n",
    "  model = svm.SVC(kernel='rbf') \n",
    "  model.fit(X_train, y_train) \n",
    "  predicted_sentiment = model.predict(X_test)\n",
    "  a = accuracy_score(y_test, predicted_sentiment)\n",
    "  print(a)\n",
    "  acc += a\n",
    "print(acc/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-948e0cf6bac6>, line 98)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-948e0cf6bac6>\"\u001b[1;36m, line \u001b[1;32m98\u001b[0m\n\u001b[1;33m    print(a)\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from dataScript import getDataset\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = getDataset()\n",
    "acc = 0\n",
    "for j in range(5):\n",
    "  data = data.sample(frac=1).reset_index(drop=True)\n",
    "  data.head()\n",
    "  #data.Sentiment.value_counts().plot(kind='pie', autopct='%1.0f%%', colors=[\"red\", \"yellow\", \"green\"])\n",
    "\n",
    "  features = data.iloc[:,0].values\n",
    "  labels = data.iloc[:,1].values\n",
    "  lenth = 0\n",
    "  for sen in features:\n",
    "      words = word_tokenize(sen)\n",
    "      if len(words) > lenth:\n",
    "          lenth = len(words)\n",
    "\n",
    "  print(lenth)\n",
    "\n",
    "\n",
    "  f = open(\"stop_words.txt\", \"r\", encoding='utf8')\n",
    "  stop = []\n",
    "  for x in f:\n",
    "    stop.append(x.replace('\\n',''))\n",
    "  f.close()\n",
    "  processed_features = []\n",
    "  ss = []\n",
    "\n",
    "  for i in range(0, len(stop)):\n",
    "      s = str(stop[i])\n",
    "      s = s.lower()\n",
    "      s= re.sub(r'ü', 'u', s)\n",
    "      s= re.sub(r'ö', 'o', s)\n",
    "      s= re.sub(r'ı', 'i', s)\n",
    "      s= re.sub(r'ğ', 'g', s)\n",
    "      s= re.sub(r'ç', 'c', s)\n",
    "      ss.append(s)\n",
    "\n",
    "  for sentence in range(0, len(features)):\n",
    "      processed_feature = re.sub(r'\\s\\'\\s', '\\'', str(features[sentence]))\n",
    "      processed_feature= re.sub(r'[.!?,\\\"\\[\\]]', '', processed_feature)\n",
    "      #processed_feature= re.sub(r'(?<=\\d)(\\'\\s)', '\\'', processed_feature)    \n",
    "      processed_feature = processed_feature.lower()\n",
    "      processed_feature= re.sub(r'ü', 'u', processed_feature)\n",
    "      processed_feature= re.sub(r'ö', 'o', processed_feature)\n",
    "      processed_feature= re.sub(r'ı', 'i', processed_feature)\n",
    "      processed_feature= re.sub(r'ğ', 'g', processed_feature)\n",
    "      #processed_feature= re.sub(r'ş', 's', processed_feature)\n",
    "      processed_feature= re.sub(r'ç', 'c', processed_feature)\n",
    "      # Replaces #hashtag with hashtag\n",
    "      processed_feature = re.sub(r'#(\\S+)', r' \\1 ', processed_feature)\n",
    "      # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "      processed_feature = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', processed_feature)\n",
    "      # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "      processed_feature = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', processed_feature)\n",
    "      # Love -- <3, :*\n",
    "      processed_feature = re.sub(r'(<3|:\\*)', ' EMO_POS ', processed_feature)\n",
    "      # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "      processed_feature = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', processed_feature)\n",
    "      # Sad -- :-(, : (, :(, ):, )-:\n",
    "      processed_feature = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', processed_feature)\n",
    "      # Cry -- :,(, :'(, :\"(\n",
    "      processed_feature = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', processed_feature)\n",
    "\n",
    "      # Convert more than 2 letter repetitions to 2 letter\n",
    "      processed_feature = re.sub(r'(.)\\1+', r'\\1\\1', processed_feature)\n",
    "      processed_features.append(processed_feature)\n",
    "  #print(processed_features[10:])\n",
    "  # all_words = []\n",
    "  # tknzr = TweetTokenizer()\n",
    "  # for sen in processed_features:\n",
    "  #     words = tknzr.tokenize(sen)    \n",
    "  #     #print(words)\n",
    "  #     filtered_words = [w for w in words if not w in stop]\n",
    "  #     all_words.append(str(filtered_words))\n",
    "  #     #print(filtered_words)\n",
    "  \n",
    "  vectorizer = TfidfVectorizer (max_features=2500, min_df=2, max_df=0.5, stop_words=stop)\n",
    "  processed_features = vectorizer.fit_transform(processed_features)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=1\n",
    "  \n",
    "\n",
    "\n",
    "  print(a)\n",
    "  acc += a\n",
    "print(acc/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}